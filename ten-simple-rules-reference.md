## Disclaimer

This reference document was formed by taking large excerpts from the paper "Ten Simple Rules for AI-Assisted Coding in Science" by Eric W. Bridgeford, Iain Campbell, Zijiao Chen, Zhicheng Lin, Harrison Ritz, Joachim Vandekerckhove, and Russell A. Poldrack. The original paper provides comprehensive discussion, references, and appendices.

# (An Abbreviated Version of) Ten Simple Rules for AI-Assisted Coding in Science

## Introduction

The integration of artificial intelligence into scientific computing represents one of the most significant shifts in research methodology in recent years. Large language models (LLMs) can now generate syntactically correct, functionally appropriate programs from natural language descriptions. Tools like GitHub Copilot, ChatGPT, and Claude have democratized access to sophisticated programming assistance, enabling researchers with limited coding experience to implement complex analyses and build robust scientific software.

However, while AI-assisted coding tools have demonstrated measurable productivity gains in some controlled studies, the evidence remains contested and situation-dependent. These questions become particularly acute in scientific computing, where code is not merely a means to an end but often embodies scientific reasoning, methodological decisions, and domain expertise. The validity, reproducibility, and interpretability of scientific software directly impact research integrity and the reliability of scientific findings.

Effective use of AI coding tools requires understanding how to work within and around their constraints. The ten rules presented here emerge from collective experience using AI-assisted coding tools, and highlight both the substantial promise and documented risks of AI-assisted coding in scientific settings. These guidelines emphasize the importance of maintaining human agency in the coding process, establishing robust testing and validation procedures, and strategically managing the interaction between human expertise and AI assistance.

## The Rules

### Preparation and Understanding

The effectiveness of AI-assisted coding is fundamentally constrained by the clarity and completeness of what you bring to the interaction. AI models cannot compensate for gaps in your understanding of the problem domain, your inability to recognize appropriate solutions, or your unfamiliarity with relevant tools and conventions. These limitations arise because AI tools lack true domain expertise. They pattern-match from training data rather than reason from first principles, and they cannot assess whether their outputs align with field-specific best practices you haven't mentioned. Without this foundational preparation, you risk "vibe coding," accepting AI-generated code you cannot evaluate, debug, or maintain.

**Rule 1: Gather Domain Knowledge Before Implementation**

Know your problem space before writing code. Understand data shapes, missing data patterns, field-specific libraries, and existing implementations that could serve as models. You don't need to be an expert initially; use AI to help research domain standards, available datasets, common approaches, and implementation patterns before diving into coding. This reconnaissance phase prevents you from reinventing wheels or violating field conventions. Share your current understanding level with the AI and iteratively build context through targeted questions about tools, data structures, and best practices, asking for specific references and paper summaries. This upfront investment ensures that your code aligns with community standards and handles real-world data appropriately.

**Rule 2: Distinguish Problem Framing from Coding**

Framing a problem in a programmatic way and coding are not the same thing. Programmatic problem framing is problem solving: understanding the domain, decomposing complex problems, finding the right levels of abstraction, designing algorithms, and making architectural decisions. Coding is the mechanical translation of these concepts into executable syntax in a programming language. Using AI coding tools effectively requires that you deeply understand the problem from a programmatic perspective that you are trying to solve; in most cases this understanding transcends the particular programming language, and the actual code implementation itself. AI tools excel at coding tasks, generating syntactically correct implementations from well-specified requirements, but they currently require human guidance for programmatic problem framing decisions that involve domain expertise, methodological choices, and scientific reasoning. You can't effectively guide or review what you don't understand, so establish fluency in at least one programming language and fundamental concepts before leveraging AI assistance. This foundation allows you to spot when generated code deviates from best practices or introduces subtle bugs. Without this knowledge, you're essentially flying blind, unable to distinguish between elegant solutions and convoluted workarounds.

**Rule 3: Choose Appropriate AI Interaction Models**

It's tempting to use the AI tools to independently generate a complete codebase, but one quickly ends up being separated from the code and making mistakes. A pair programming model, where one directs interactive AI assistants through comments in the code, can be a way to stay in close touch with the evolution of the codebase. The utility of different language models and tool types will depend on the specific development tasks, developer preferences, and project constraints.

Different AI-assisted development tools are categorized by interaction model and deployment scenario. Conversational tools like ChatGPT and Claude are best for architecture design, complex debugging, and learning new concepts, offering deep reasoning and flexible problem-solving with extensive context handling, but requiring manual code transfer and losing context between sessions. IDE assistants like Copilot and IntelliSense excel at code completion, refactoring, and maintaining flow through seamless workflow integration with immediate feedback and preserved code context, but have limited reasoning for complex architectural decisions. Autonomous agents like Cursor, Claude Code, and Aider are ideal for rapid prototyping, multi-file changes, and large refactoring, providing high-speed implementation that can work independently across multiple files, but risk code divergence and require careful monitoring.

### Context Engineering & Interaction

Once you understand your problem domain and have chosen appropriate tools, effective AI collaboration requires careful management of how you structure prompts and maintain context across interactions. Most AI systems are stateless, forgetting previous conversations, while others struggle with context rot as conversations grow long. This creates two critical challenges: ensuring the AI has all necessary information to generate appropriate code, and recognizing when a conversation has become too polluted with failed attempts to be productive.

**Rule 4: Start by Thinking Through a Potential Solution**

Begin AI coding sessions by first working to understand and articulate the problem you're trying to solve, specified at a level of abstraction that makes it solvable by code, and think through how you anticipate it might be solved. Think through the entire problem space: What are the inputs and expected outputs? What are the key constraints and edge cases? What does success look like? This problem-focused approach serves a dual purpose: it helps you clarify exactly what you want the AI to accomplish so that you can evaluate its outputs appropriately, and it prevents the AI from making incorrect assumptions about your goals. When you provide problem context along with architectural details of how you anticipate a solution working (i.e., how it might fit in with the "bigger picture"), the AI generates code that fits naturally into your project rather than creating isolated solutions. Include details about data flow, component interactions, and expected interfaces when possible or if relevant. This approach transforms the AI from a code generator into an architecture-aware development partner. You can use LLMs to help generate externally-managed context files, and also look at GitHub Spec Kit for specification-driven workflows that define project requirements and gated phases (Specify, Plan, Tasks). AI can help you implement sophisticated patterns like structured checklists for iterative development that would be tedious to write from scratch.

**Rule 5: Manage Context Strategically**

Context (which for LLMs refers to all of the information currently in the model's equivalent of "working memory") is everything in AI-assisted coding. Provide all necessary information upfront through clear documentation, attached references, or structured project files with dependencies included. Don't assume the AI retains perfect context across long conversations; explicitly restate critical requirements, constraints, and dependencies when interactions get complex. Keep track of context and clear or compact when it's getting close to limits. Use externally-managed context files to keep important context available across sessions while minimizing irrelevant details that can degrade AI performance. Agents can effectively use these files to keep important things in context for every session. It's also useful to keep a problem solving file, where you can add problems whenever you notice them, and where the model can keep track of its progress.

### Testing & Validation

Testing becomes even more critical when AI generates implementation code. While AI can accelerate code generation, it cannot be trusted to ensure correctness, handle edge cases appropriately, or validate that code meets scientific standards. The rules in this section establish practices for using tests as specifications that guide AI code generation, and for leveraging AI to build and improve comprehensive test coverage.

**Rule 6: Implement Test-Driven Development with AI**

Frame your test requirements as behavioral specifications before requesting implementation code, and tell the AI what success looks like through concrete test cases. This test-first approach forces you to articulate edge cases, expected inputs/outputs, and failure modes that might otherwise be overlooked. AI will respond better to specific test scenarios than vague functionality descriptions. By providing comprehensive test specifications, you guide the AI toward more robust, production-ready implementations. AI tools (such as chatbots or Github's Spec Kit) can help develop these specifications in a way that will optimally guide the model. Keep a close eye on the tests that are generated, since the models will often modify the tests to pass without actually solving the problem rather than generating suitable code. Be especially aware that coding agents may generate placeholder data or mock implementations that merely satisfy the test structure without validating actual logic. In many cases, the AI may insert fabricated input values or dummy functions that appear to meet acceptance criteria but do not reflect true functionality. These "paper tests" can be dangerously misleading, seemingly passing as tests while masking broken or incomplete logic. In addition, whenever a bug is identified during your development cycle, ask the model to generate a test that catches the bug, to ensure that it's not re-introduced in the future.

**Rule 7: Leverage AI for Test Planning and Refinement**

AI is exceptionally good at identifying edge cases you might miss and suggesting comprehensive test scenarios. Feed it your function and ask it to generate tests for boundary conditions, type validation, error handling, and numerical stability. Ask it what sorts of problems your code might experience issues with, within your specified API bounds, and why those might (or might not) be relevant to address. AI can help you move beyond testing only expected behavior to robust validation that includes malformed inputs, extreme values, and unexpected conditions. Additionally, you can use AI to review your existing tests and identify gaps in coverage or scenarios you haven't considered. The AI can help you implement sophisticated testing patterns like parameterized tests, fixtures, and mocking that would be tedious to write from scratch. If you anticipate having future collaborators for your project, you may find it helpful to prioritize building testing infrastructure early. This often includes automated validation workflows, wherein you are able to test your code automatically as you integrate changes into the broader project. AI excels at generating the boilerplate for many of these sophisticated testing tools (such as GitHub Actions, pre-commit hooks, and test orchestration) that ensure your code is validated on every push.

### Code Quality & Validation

Even with comprehensive tests, AI-generated code requires careful human oversight to ensure correctness and appropriateness. AI models can confidently produce code that passes tests but violates domain conventions, introduces subtle bugs, or solves problems in scientifically inappropriate ways. The final rules address critical aspects of quality assurance: actively monitoring AI progress to catch problems early, and critically reviewing and continuing to iterate with all generated code to ensure it meets scientific standards.

**Rule 8: Monitor Progress and Know When to Restart**

It's tempting to just walk away and let the model work for a long time, but often the model will end up going down the wrong path, wasting time and tokens. You need to actively monitor what the AI is doing: Is it changing things you didn't want changed? Is it ignoring the changes you actually requested? Is it introducing new problems while trying to fix old ones? When you notice the AI heading in the wrong direction, stop it rather than letting it continue down an unproductive path. Further, sometimes the most efficient approach is recognizing when a conversation has become too convoluted with failed attempts. When this happens, review your prompt history to identify what went wrong: Were requirements unclear? Did you add conflicting constraints? Did you forget to specify critical details upfront? Starting fresh with these lessons learned often produces better results than continuing to debug within a polluted context. Clear the context and restart from externally-managed context files after adding additional details to prevent the same problem from occurring in the future. This also highlights the need for good version control; if you commit code before undertaking a major change, it's easy to simply revert to the previous commit and start over if the model goes astray. Fortunately coding agents are generally very good at writing detailed commit messages, making a commit as easy as prompting "commit this to git".

**Rule 9: Critically Review Generated Code**

Be skeptical about AI's claims of success; the models tend to claim success even when they haven't really solved the problem. You always need to test the solution independently. Read and understand the code to ensure it solves problems in ways that make sense for your domain and match your prior expectation of how the problem should be solved (e.g., how you anticipated a solution looking based on your pseudocode or architecture schematics you developed in Rule 4). AI-generated code requires careful human review to ensure scientific appropriateness, methodological soundness, and alignment with domain standards.

**Rule 10: Refine Code Incrementally with Focused Objectives**

Once you have working, tested code, resist the temptation to ask AI to "improve my codebase." Instead, approach refinement incrementally with clear, focused objectives. Be explicit about what aspect you want to improve: performance optimization, code readability, error handling, modularity, or adherence to specific design patterns. When you recognize that refinement is needed but can't articulate the specific approach (for instance, you know certain logic should be extracted into a separate function but aren't sure how), use AI to help you formulate concrete objectives before implementing changes. Describe what you are trying to achieve and ask the AI to suggest specific refactoring strategies or design patterns that would accomplish your goal, applying the same mindsets delineated in Rules 1-9 to help you along the way.

AI excels at identifying opportunities for refactoring and abstraction, such as recognizing repeated code that should be extracted into reusable functions or methods, and detecting poor programming patterns like deeply nested conditionals, overly long functions, tight coupling between components, sloppy or inconsistent variable naming conventions, and other poor patterns. When requesting refinements, specify the goal (e.g., "extract the data validation logic into a separate function" rather than "make this better") and verify each change against your tests (while expanding your testing as you iterate to reflect updates and improvements) before moving to the next improvement. This focused approach prevents the AI from making changes that, while technically sound, don't align with your project's architectural decisions. Note that AI can inadvertently break previously working code or degrade performance while making stylistic improvements. Always test thoroughly after each incremental change, and revert if the "improvement" introduces problems or doesn't provide clear benefits.

## Discussion

This document presents ten rules for leveraging AI coding tools effectively in scientific computing while maintaining methodological rigor and code quality. These rules are organized around four key themes: preparation and understanding, context engineering, testing and validation, and code quality assurance. However, we acknowledge a fundamental reality based on our experiences: even when following these rules, flawless start-to-finish interactions are the exception rather than the norm. The value of these rules lies not in guaranteeing immediate success, but in providing a framework that helps you focus on what matters most for successful interactions while also enabling you to quickly diagnose what went wrong when interactions fail, so you can iterate more effectively on your next attempt.

### Ethical Considerations and Responsibility

The use of AI-assisted coding raises fundamental questions about scientific accountability. When code that generates published results is partly AI-generated, who bears responsibility for errors, methodological flaws, or irreproducible outcomes? The answer must be unequivocal: the scientist. AI tools are instruments, and like any instrument in science, the researcher using them remains fully accountable for validating their outputs and ensuring methodological soundness. This responsibility cannot be delegated to the AI, regardless of how sophisticated the tool or how confident its outputs appear. Researchers must ensure their AI-assisted code is reproducible, well-documented, and scientifically appropriate. When AI generates code that implements a statistical method or analytical pipeline, the researcher must understand that implementation well enough to defend its appropriateness, explain its limitations, and troubleshoot unexpected results. "AI wrote it" is not a valid defense for flawed methodology or incorrect results. Transparency about AI usage in methods sections, while important, does not diminish this responsibility.

Beyond individual accountability, broader ethical concerns demand serious consideration. The environmental costs of training and running large language models are substantial and measurable. These systems consume enormous amounts of energy and computational resources, raising questions about the sustainability of widespread AI adoption. Further, intellectual property questions surrounding AI training on open-source code and the ownership of AI-generated code remain legally and ethically unsettled. Courts have yet to definitively rule on whether training on copyrighted code constitutes fair use, whether AI-generated code can be copyrighted, and who owns the rights to such code when models have been trained on proprietary or licensed material. These are fundamental ethical and legal challenges that the scientific community must grapple with as AI tools become embedded in research infrastructure. While these complex issues merit a dedicated treatment beyond our scope here, researchers should recognize that using AI coding tools involves participating in systems with significant unresolved ethical dimensions.

### Guardrails for Autonomous Agents

Autonomous coding agents can make extensive changes across a codebase with minimal human intervention, dramatically accelerating development but introducing risks if not properly constrained. The primary danger lies in granting agents too much control without appropriate safeguards. An agent given broad permissions might break existing functionality, introduce security vulnerabilities, or violate architectural principles while reporting success.

We recommend several guardrails. First, use containerized or sandboxed environments for agent-driven development, isolating agent operations from production systems. Second, commit working code before allowing agent changes, enabling easy rollback. Third, learn how to properly configure agents with explicit constraints about what they can modify and what actions require human approval. Fourth, maintain active monitoring rather than allowing unsupervised operation, as discussed in Rule 8. For individual projects, consider project-specific containers where each agent operates in an isolated environment with restricted file access. As autonomous agents become more capable, developing clear and safe practices for constraining and monitoring their behavior will become increasingly important for maintaining scientific rigor and system safety.

### Limitations and Future Directions

We acknowledge that we are operating in a rapidly evolving technological landscape. For reference, GPT-3 (2020) had a context window of 2,048 tokens, GPT-4 (2023) expanded this to variants with tens of thousands of tokens, and current state-of-the-art models like Gemini 2.5 Pro (2025) can operate with context windows of millions of tokens. In light of this rapid evolution, we have intentionally focused on principles and practices that remain relevant across different AI capabilities. We believe our proposed rules emphasize fundamental skills (domain knowledge, problem decomposition, critical review) and strategies (context management, test-driven development, incremental refinement) that apply regardless of specific tools, and thus far have proven useful throughout the evolution of AI models to-date. We have deliberately avoided prescriptive recommendations and strategies tied to specific models, as these would quickly become outdated. Future advances may change which practices prove most valuable, but we believe these rules provide a useful framework for current practice that will remain adaptable as technology matures.

We also anticipate substantial evolution in how scientists acknowledge the role of AI in their work. As AI coding becomes standard practice, we expect clearer community expectations for documenting AI tool usage and validating AI-generated code; this may include citations of specific systems, disclosure of prompting approaches, detailed validation procedures in methods sections, and heightened expectations regarding testing, validation, and reproducibility of code derivatives. The practices we recommend (systematic context building, comprehensive testing, and critical validation) may provide a foundation for informing and meeting these emerging accountability standards.

## Acknowledgments

An initial framework of 20 rules and focus points was developed by the original authors, which were streamlined into 10 rules with assistance from Claude (Anthropic). The rules were then authored and iteratively refined by the research team. This work was supported by a grant from the Sloan Foundation to RAP (G-2025-25270).

The interactive Jupyter Book with detailed examples is available at [poldracklab.org/10sr_ai_assisted_coding](https://poldracklab.org/10sr_ai_assisted_coding).
